Index: SingleAgentCombat-main/main_only_blue.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nfrom matplotlib import style\r\nfrom tqdm import tqdm\r\n\r\nstyle.use(\"ggplot\")\r\nfrom Arena.CState import State\r\nfrom Arena.Entity import Entity\r\nfrom Arena.Environment import Environment, Episode\r\nfrom Common.constants import *\r\nfrom Qtable import Qtable_DecisionMaker\r\nfrom DQN import DQNAgent_keras\r\nfrom Greedy import Greedy_player\r\n\r\n\r\ndef print_start_of_game_info(blue_decision_maker, red_decision_maker):\r\n    print(\"Starting tournament!\")\r\n    print(\"Blue player type: \", Agent_type_str[blue_decision_maker.type()])\r\n    if blue_decision_maker.path_model_to_load==None:\r\n        print(\"Blue player starting with no model\")\r\n    else:\r\n        print(\"Blue player starting tournament with trained model: \" , blue_decision_maker.path_model_to_load)\r\n\r\n    print(\"Red player type: \", Agent_type_str[red_decision_maker.type()])\r\n    if red_decision_maker.path_model_to_load==None:\r\n        print(\"Red player starting with no model\")\r\n    else:\r\n        print(\"Red player starting tournament with trained model: \" , red_decision_maker.path_model_to_load)\r\n\r\n\r\n    print(\"Number of rounds: \", NUM_OF_EPISODES)\r\n    print(\"~~~ GO! ~~~\\n\\n\")\r\n\r\n\r\ndef evaluate(episode_number):\r\n    if episode % EVALUATE_PLAYERS_EVERY == 0:\r\n        EVALUATE = True\r\n    else:\r\n        EVALUATE = False\r\n    return EVALUATE\r\n\r\n\r\n# MAIN:\r\nif __name__ == '__main__':\r\n\r\n    env = Environment(IS_TRAINING)\r\n\r\n    ### Red Decision Maker\r\n    #red_decision_maker = Qtable_DecisionMaker.Qtable_DecisionMaker()\r\n    red_decision_maker = Greedy_player.Greedy_player()\r\n    #red_decision_maker = Qtable_DecisionMaker.Qtable_DecisionMaker(UPDATE_CONTEXT=False , path_model_to_load=\"qtable_red-1000000_penalty_move_-1.pickle\")\r\n    #red_decision_maker = DQNAgent_keras.DQNAgent_keras(UPDATE_CONTEXT=False,\r\n    #                                                    path_model_to_load='flatten_FC1-elu_FC2-elu_FC3-elu_FC4-elu__red_25001_  -6.00max_ -72.99avg_-100.00min__1615541339.model')\r\n    #red_decision_maker = DQNAgent_keras.DQNAgent_keras()\r\n\r\n    ### Blue Decision Maker\r\n    #--Greedy:\r\n    #blue_decision_maker = Greedy_player.Greedy_player()\r\n    # --Qtable:\r\n    blue_decision_maker = Qtable_DecisionMaker.Qtable_DecisionMaker()\r\n    #blue_decision_maker = Qtable_DecisionMaker.Qtable_DecisionMaker('qtable_blue-600000_penalty_move_-1.pickle')\r\n    # --DQN Basic:\r\n    #blue_decision_maker = DQNAgent.DQNAgent()\r\n    # blue_decision_maker = DQNAgent.DQNAgent(UPDATE_CONTEXT=False, path_model_to_load='basic_DQN_17500_blue.model')\r\n    # --DQN Keras\r\n    #blue_decision_maker = DQNAgent_keras.DQNAgent_keras()\r\n    #blue_decision_maker = DQNAgent_keras.DQNAgent_keras(UPDATE_CONTEXT=True, path_model_to_load='flatten_FC1-elu_FC2-elu_FC3-elu_FC4-elu__blue_20001_ 116.00max_ -99.54avg_-135.00min__1616232409.model')\r\n    #flatten_FC1-elu_FC2-elu_FC3-elu_FC4-elu__blue_30001_ 120.00max_  97.59avg_-100.00min__1615828123\r\n    # blue_decision_maker = DQNAgent_keras.DQNAgent_keras(UPDATE_CONTEXT=True,\r\n    #                                                     path_model_to_load='flatten_FC1-elu_FC2-elu_FC3-elu_FC4-elu__blue_157501_ 120.00max_   4.80avg_   0.00min__1615952583.model')\r\n\r\n    # --DQN Attention\r\n    # blue_decision_maker = DQNAgent_spatioalAttention.DQNAgent_spatioalAttention()\r\n    # blue_decision_maker = DQNAgent_spatioalAttention.DQNAgent_spatioalAttention(UPDATE_CONTEXT=True, path_model_to_load='statistics/18_02_06_54_DQNAgent_spatioalAttention_Q_table_1000000/qnet1000000.cptk')\r\n    # blue_decision_maker = DQNAgent_temporalAttention.DQNAgent_temporalAttention()\r\n\r\n\r\n    env.blue_player = Entity(blue_decision_maker)\r\n    env.red_player = Entity(red_decision_maker)\r\n\r\n    print_start_of_game_info(blue_decision_maker, red_decision_maker)\r\n\r\n    NUM_OF_EPISODES = env.NUMBER_OF_EPISODES\r\n    for episode in tqdm(range(1, NUM_OF_EPISODES + 1), ascii=True, unit='episodes'):\r\n\r\n        current_episode = Episode(episode, show_always=False if IS_TRAINING else True)\r\n\r\n        # set new start position for the players\r\n        env.reset_players_positions(episode)\r\n\r\n        # get observation\r\n        observation_for_blue_s0: State = env.get_observation_for_blue()\r\n\r\n\r\n        action_blue = -1#AgentAction(np.random.randint(0, NUMBER_OF_ACTIONS))\r\n\r\n\r\n        # initialize the decision_makers for the players\r\n        blue_decision_maker.set_initial_state(observation_for_blue_s0, episode)\r\n        #red_decision_maker.set_initial_state(observation_for_red_s0, episode) # for non-greedy players\r\n\r\n        EVALUATE = evaluate(episode)\r\n\r\n        blue_won_the_game = False\r\n        red_won_the_game = False\r\n        for steps_current_game in range(1, MAX_STEPS_PER_EPISODE + 1):\r\n\r\n\r\n            ##### Blue's turn! #####\r\n            observation_for_blue_s0: State = env.get_observation_for_blue()\r\n            current_episode.print_episode(env, steps_current_game)\r\n\r\n            action_blue: AgentAction = blue_decision_maker.get_action(observation_for_blue_s0, EVALUATE)\r\n            env.blue_player.action(action_blue)  # take the action!\r\n            current_episode.print_episode(env, steps_current_game)\r\n\r\n            current_episode.is_terminal = (env.compute_terminal(whos_turn=Color.Blue) is not WinEnum.NoWin)\r\n\r\n            #if not terminal: check if red can win in next state\r\n            if not current_episode.is_terminal:\r\n                can_red_win, lossing_blue_state_obs = env.can_red_win()\r\n                if can_red_win:\r\n                    reward_step_blue, reward_step_red = env.handle_reward(steps_current_game,\r\n                                                                          can_red_win,\r\n                                                                          whos_turn=Color.Red)\r\n                    # Update model for Blue player\r\n                    assert reward_step_blue == -WIN_REWARD\r\n                    blue_decision_maker.update_context(observation_for_blue_s0, action_blue, lossing_blue_state_obs,\r\n                                                       reward_step_blue, can_red_win, EVALUATE)\r\n\r\n                    if False:\r\n                        import matplotlib.pyplot as plt\r\n\r\n                        plt.matshow(observation_for_blue_s0.img)\r\n                        plt.show()\r\n\r\n                        plt.matshow(lossing_blue_state_obs.img)\r\n                        plt.show()\r\n\r\n                    current_episode.episode_reward_blue += reward_step_blue\r\n                    current_episode.print_episode(env, steps_current_game)\r\n                    break #inbal: should break here???\r\n\r\n\r\n\r\n            reward_step_blue, reward_step_red = env.handle_reward(steps_current_game,\r\n                                                                 current_episode.is_terminal,\r\n                                                                 whos_turn=Color.Blue)\r\n\r\n            current_episode.episode_reward_red += reward_step_red\r\n\r\n\r\n            if current_episode.is_terminal:# Blue won the game!\r\n                env.update_win_counters(steps_current_game, whos_turn=Color.Blue)\r\n                current_episode.episode_reward_blue += reward_step_blue\r\n\r\n                # env.red_player.action(AgentAction(np.random.randint(0, NUMBER_OF_ACTIONS)))\r\n\r\n\r\n                # Update model for Blue player- red player doesnt move!\r\n                observation_for_blue_s1: State = env.get_observation_for_blue()\r\n                assert reward_step_blue == WIN_REWARD\r\n                blue_decision_maker.update_context(observation_for_blue_s0, action_blue, observation_for_blue_s1,\r\n                                                   reward_step_blue, current_episode.is_terminal, EVALUATE)\r\n                if False:\r\n                    import matplotlib.pyplot as plt\r\n\r\n                    plt.matshow(observation_for_blue_s0.img)\r\n                    plt.show()\r\n                    plt.matshow(observation_for_blue_s1.img)\r\n                    plt.show()\r\n                current_episode.print_episode(env, steps_current_game)\r\n                break\r\n\r\n\r\n            ##### Red's turn! #####\r\n            observation_for_red_s0: State = env.get_observation_for_red()\r\n            action_red: AgentAction = red_decision_maker.get_action(observation_for_red_s0, EVALUATE)\r\n            env.red_player.action(action_red)  # take the action!\r\n\r\n            observation_for_blue_s1: State = env.get_observation_for_blue()\r\n            current_episode.print_episode(env, steps_current_game)\r\n\r\n\r\n            current_episode.is_terminal = (env.compute_terminal(whos_turn=Color.Red) is not WinEnum.NoWin)\r\n            reward_step_blue, reward_step_red = env.handle_reward(steps_current_game,\r\n                                                                 current_episode.is_terminal,\r\n                                                                 whos_turn=Color.Red)\r\n\r\n            # Update model for Blue player\r\n            # blue did not win last move, and red did not (and could not) win last move\r\n            assert reward_step_blue==-MOVE_PENALTY or reward_step_blue==-WIN_REWARD\r\n            blue_decision_maker.update_context(observation_for_blue_s0, action_blue, observation_for_blue_s1,\r\n                                               reward_step_blue, current_episode.is_terminal, EVALUATE)\r\n\r\n            current_episode.episode_reward_blue += reward_step_blue\r\n            if current_episode.is_terminal:\r\n                env.update_win_counters(steps_current_game, whos_turn=Color.Red)\r\n\r\n                # # Update model for Red player\r\n                # red_decision_maker.update_context(observation_for_red_s0, action_red, observation_for_red_s1,\r\n                #                                   reward_step_red,\r\n                #                                   current_episode.is_terminal, EVALUATE)\r\n\r\n\r\n                current_episode.episode_reward_red += reward_step_red\r\n                current_episode.print_episode(env, steps_current_game)\r\n                break\r\n\r\n\r\n\r\n\r\n            if steps_current_game == MAX_STEPS_PER_EPISODE:\r\n                # if we exited the loop because we reached MAX_STEPS_PER_EPISODE\r\n                current_episode.is_terminal = True\r\n                env.update_win_counters(steps_current_game)\r\n                break\r\n\r\n\r\n        # for statistics\r\n        env.data_for_statistics(current_episode.episode_reward_blue, current_episode.episode_reward_red, steps_current_game, blue_decision_maker.get_epsolon())\r\n\r\n        # print info of episode:\r\n        current_episode.print_info_of_episode(env, steps_current_game, blue_decision_maker.get_epsolon())\r\n\r\n    env.end_run()\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/SingleAgentCombat-main/main_only_blue.py b/SingleAgentCombat-main/main_only_blue.py
--- a/SingleAgentCombat-main/main_only_blue.py	(revision b91383db04bba11b135a1f0b838bc21ccf0c0d45)
+++ b/SingleAgentCombat-main/main_only_blue.py	(date 1616524948959)
@@ -142,6 +142,7 @@
 
 
 
+
             reward_step_blue, reward_step_red = env.handle_reward(steps_current_game,
                                                                  current_episode.is_terminal,
                                                                  whos_turn=Color.Blue)
